{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "from mdp import TradeExecutionEnv, DiscreteTradeSizeWrapper, RelativeTradeSizeWrapper\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "HORIZON = 5 * 12 * 2\n",
    "UNITS_TO_SELL = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 300\n",
    "TASK_BATCH_SIZE = 12\n",
    "EPSILON = 0.1\n",
    "GAMMA = 0.99\n",
    "\n",
    "env = TradeExecutionEnv()\n",
    "\n",
    "trade_sizes = {\n",
    "  i: i*2 for i in range(33)\n",
    "}\n",
    "env = DiscreteTradeSizeWrapper(env, trade_sizes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32) -> None:\n",
    "        super().__init__()\n",
    "        #self.rnn = nn.LSTM(num_states, hidden_dim, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x, _ = self.rnn(x)\n",
    "        x = F.relu(self.fc1(x[:, -1, :]))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(state):\n",
    "    data = torch.FloatTensor(np.stack([\n",
    "        state[\"low\"].to_numpy(),\n",
    "        state[\"high\"].to_numpy(),\n",
    "        state[\"close\"].to_numpy(),\n",
    "        state[\"open\"].to_numpy(),\n",
    "        state[\"volume\"].to_numpy(),\n",
    "    ])).T\n",
    "    return torch.concat([\n",
    "        data,\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"units_sold\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"cost_basis\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"steps_left\"]]]), 6, 0),\n",
    "    ], dim=1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rollouts(policy, env, num_rollouts, seed, test=False):\n",
    "    trajs = []\n",
    "    for _ in range(num_rollouts):\n",
    "        tau = []\n",
    "        state = env.reset(UNITS_TO_SELL, HORIZON, seed=seed, test=test)\n",
    "        state = parse_state(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = parse_state(next_state)\n",
    "            tau.append((state, action, reward))\n",
    "            state = next_state\n",
    "        states, actions, rewards = zip(*tau)\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor([actions])\n",
    "        rewards = torch.tensor([rewards])\n",
    "        trajs.append((states, actions, rewards))\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_tau(r, gamma):\n",
    "    gammas = torch.tensor([gamma**i for i in range(len(r))])\n",
    "    return torch.sum(gammas * r)\n",
    "\n",
    "def avg_batch_rewards(trajs, gamma):\n",
    "    return torch.mean(torch.stack([R_tau(r, gamma) for _, _, r in trajs]))\n",
    "\n",
    "def grad_log_pi(policy, states, actions):\n",
    "    logits = policy(states)\n",
    "    log_probs = torch.log(logits)\n",
    "    log_probs = log_probs.gather(1,actions).T.flatten()\n",
    "    g = [torch.autograd.grad(log_p, policy.parameters(), retain_graph=True) for log_p in log_probs]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_tau(policy, tau):\n",
    "  g = grad_log_pi(policy, tau[0], tau[1])\n",
    "  stacked_g = [torch.stack(g_).view(len(tau[0]), -1) for g_ in zip(*g)]\n",
    "  return torch.cat(stacked_g, axis=1).sum(axis=0)\n",
    "\n",
    "def grad_U_tau(policy, tau, gamma):\n",
    "  return grad_log_tau(policy, tau) * R_tau(tau[2], gamma)\n",
    "\n",
    "def flatten_params(policy):\n",
    "    return torch.cat([p.view(-1) for p in policy.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: -8.880998514508157\n",
      "iter 1: -9.226531727473407\n",
      "iter 2: -9.13046766528586\n",
      "iter 3: -9.261844773919554\n",
      "iter 4: -9.188043071530375\n",
      "iter 5: -9.254966896620909\n",
      "iter 6: -9.331928828489351\n",
      "iter 7: -9.050284972335803\n",
      "iter 8: -9.18257610640031\n",
      "iter 9: -9.06741719637562\n",
      "iter 10: -9.405331208400106\n",
      "iter 11: -9.193426984781903\n",
      "iter 12: -9.267656134341593\n",
      "iter 13: -9.237012758763013\n",
      "iter 14: -8.959855212670048\n",
      "iter 15: -9.432641179237512\n",
      "iter 16: -9.382913497791344\n",
      "iter 17: -9.182923686169902\n",
      "iter 18: -9.387646518691456\n",
      "iter 19: -9.335296275257376\n",
      "iter 20: -9.368544426075651\n",
      "iter 21: -8.935014013022231\n",
      "iter 22: -8.755760646021107\n",
      "iter 23: -8.922913066498591\n",
      "iter 24: -8.913571418431859\n",
      "iter 25: -9.147146695871465\n",
      "iter 26: -8.482045236630514\n",
      "iter 27: -9.408519642861437\n",
      "iter 28: -7.7577874050396005\n",
      "iter 29: -8.882150724565264\n",
      "iter 30: -7.800485136861173\n",
      "iter 31: -9.256521008525278\n",
      "iter 32: -8.092935946438725\n",
      "iter 33: -9.022752977957106\n",
      "iter 34: -7.8680123955167005\n",
      "iter 35: -9.205310295471458\n",
      "iter 36: -7.890873772106239\n",
      "iter 37: -7.807701226238766\n",
      "iter 38: -8.287257787404359\n",
      "iter 39: -8.093550974697687\n",
      "iter 40: -8.039457830859623\n",
      "iter 41: -8.106168197351101\n",
      "iter 42: -8.258277260932438\n",
      "iter 43: -8.045808772856855\n",
      "iter 44: -8.132790203723916\n",
      "iter 45: -8.134957914259664\n",
      "iter 46: -8.08972488703903\n",
      "iter 47: -8.073983185139488\n",
      "iter 48: -8.116800553121223\n",
      "iter 49: -7.896757787961115\n",
      "iter 50: -8.16420301490251\n",
      "iter 51: -7.813740933338093\n",
      "iter 52: -8.34413133876838\n",
      "iter 53: -7.432841744318389\n",
      "iter 54: -8.158756844825023\n",
      "iter 55: -7.973103196297234\n",
      "iter 56: -7.811736497093801\n",
      "iter 57: -8.322513098355277\n",
      "iter 58: -7.554874829682215\n",
      "iter 59: -8.453530381381023\n",
      "iter 60: -7.9121643179748995\n",
      "iter 61: -8.248708118151852\n",
      "iter 62: -9.417157606190665\n",
      "iter 63: -8.665171884672024\n",
      "iter 64: -7.896984686550058\n",
      "iter 65: -8.437314716243701\n",
      "iter 66: -8.011155296469264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m   p\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m g[n:n\u001b[39m+\u001b[39mnum_elements]\u001b[39m.\u001b[39mview(p\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m   n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_elements\n\u001b[0;32m---> 18\u001b[0m new_task_trajs \u001b[39m=\u001b[39m sample_rollouts(new_policy, env, BATCH_SIZE, seed\u001b[39m=\u001b[39mSEED\u001b[39m+\u001b[39mi)\n\u001b[1;32m     19\u001b[0m u \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([grad_U_tau(new_policy, tau, GAMMA) \u001b[39mfor\u001b[39;00m tau \u001b[39min\u001b[39;00m new_task_trajs])\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     20\u001b[0m meta_g \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m u \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m EPSILON \u001b[39m/\u001b[39m (u \u001b[39m@\u001b[39m u))\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36msample_rollouts\u001b[0;34m(policy, env, num_rollouts, seed)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m     action, _ \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mselect_action(state)\n\u001b[0;32m---> 11\u001b[0m next_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     12\u001b[0m next_state \u001b[39m=\u001b[39m parse_state(next_state)\n\u001b[1;32m     13\u001b[0m tau\u001b[39m.\u001b[39mappend((state, action, reward))\n",
      "File \u001b[0;32m~/Projects/cs238-final-project/mdp.py:106\u001b[0m, in \u001b[0;36mDiscreteTradeSizeWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrade_sizes[action])\n",
      "File \u001b[0;32m~/Projects/cs238-final-project/mdp.py:70\u001b[0m, in \u001b[0;36mTradeExecutionEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_sold \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_to_sell:\n\u001b[1;32m     69\u001b[0m     done \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_observation(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_reward(done), done, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m~/Projects/cs238-final-project/mdp.py:82\u001b[0m, in \u001b[0;36mTradeExecutionEnv._get_observation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_observation\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mopen\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_price(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step][\u001b[39m\"\u001b[39m\u001b[39mOpen\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     81\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_price(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step][\u001b[39m\"\u001b[39m\u001b[39mHigh\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m---> 82\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_price(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step][\u001b[39m\"\u001b[39m\u001b[39mLow\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     83\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mclose\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_price(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step][\u001b[39m\"\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     84\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvolume\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_volume(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39miloc[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m-\u001b[39m \u001b[39m6\u001b[39m : \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step][\u001b[39m\"\u001b[39m\u001b[39mVolume\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[1;32m     85\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39munits_sold\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_sold \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_to_sell,\n\u001b[1;32m     86\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcost_basis\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_price(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_income \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_sold) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits_sold \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m,\n\u001b[1;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msteps_left\u001b[39m\u001b[39m\"\u001b[39m: (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhorizon \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step) \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhorizon \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start)\n\u001b[1;32m     88\u001b[0m     }\n",
      "File \u001b[0;32m~/Projects/cs238-final-project/mdp.py:73\u001b[0m, in \u001b[0;36mTradeExecutionEnv.normalize_price\u001b[0;34m(self, price)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_price\u001b[39m(\u001b[39mself\u001b[39m, price):\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (price \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_price) \u001b[39m/\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_price \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_price) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39m, other)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/arraylike.py:110\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__sub__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__sub__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_arith_method(other, operator\u001b[39m.\u001b[39msub)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/series.py:6259\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_arith_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   6258\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39malign_method_SERIES(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 6259\u001b[0m     \u001b[39mreturn\u001b[39;00m base\u001b[39m.\u001b[39mIndexOpsMixin\u001b[39m.\u001b[39m_arith_method(\u001b[39mself\u001b[39m, other, op)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/base.py:1327\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   1325\u001b[0m     result \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m-> 1327\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(result, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/series.py:3224\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[39m# We do not pass dtype to ensure that the Series constructor\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[39m#  does inference in the case where `result` has object-dtype.\u001b[39;00m\n\u001b[1;32m   3223\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(result, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[0;32m-> 3224\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n\u001b[1;32m   3226\u001b[0m \u001b[39m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[39m#  would set it back to self.name\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m out\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/generic.py:5872\u001b[0m, in \u001b[0;36mNDFrame.__finalize__\u001b[0;34m(self, other, method, **kwargs)\u001b[0m\n\u001b[1;32m   5870\u001b[0m     \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata) \u001b[39m&\u001b[39m \u001b[39mset\u001b[39m(other\u001b[39m.\u001b[39m_metadata):\n\u001b[1;32m   5871\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m)\n\u001b[0;32m-> 5872\u001b[0m         \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, name, \u001b[39mgetattr\u001b[39m(other, name, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m   5874\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconcat\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   5875\u001b[0m     attrs \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mobjs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mattrs\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/cs238/lib/python3.11/site-packages/pandas/core/series.py:611\u001b[0m, in \u001b[0;36mSeries.name\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39m# DataFrame compatibility\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\n\u001b[0;32m--> 611\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    612\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mname\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Hashable:\n\u001b[1;32m    613\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39m    Return the name of the Series.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[39m    'Even Numbers'\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = CategoricalPolicy(len(env.observation_space), len(trade_sizes)).to(device)\n",
    "\n",
    "train_rewards = []\n",
    "for e in range(EPOCHS):\n",
    "  meta_g = torch.zeros_like(flatten_params(policy))\n",
    "  task_rewards = []\n",
    "  for i in range(TASK_BATCH_SIZE):\n",
    "    trajs = sample_rollouts(policy, env, BATCH_SIZE, seed=SEED+i)\n",
    "    u = torch.stack([grad_U_tau(policy, tau, GAMMA) for tau in trajs]).mean(axis=0)\n",
    "    g = u * torch.sqrt(2 * EPSILON / (u @ u))\n",
    "    n = 0\n",
    "    new_policy = CategoricalPolicy(len(env.observation_space), len(trade_sizes)).to(device)\n",
    "    new_policy.load_state_dict(policy.state_dict().copy())\n",
    "    for i, p in enumerate(new_policy.parameters()):\n",
    "      num_elements = p.numel()\n",
    "      p.data += g[n:n+num_elements].view(p.shape)\n",
    "      n += num_elements\n",
    "    new_task_trajs = sample_rollouts(new_policy, env, BATCH_SIZE, seed=SEED+i)\n",
    "    u = torch.stack([grad_U_tau(new_policy, tau, GAMMA) for tau in new_task_trajs]).mean(axis=0)\n",
    "    meta_g += u * torch.sqrt(2 * EPSILON / (u @ u))\n",
    "    policy.zero_grad()\n",
    "    task_rewards.append(avg_batch_rewards(trajs, GAMMA))\n",
    "\n",
    "  n = 0\n",
    "  meta_g /= TASK_BATCH_SIZE\n",
    "  for i, p in enumerate(policy.parameters()):\n",
    "      num_elements = p.numel()\n",
    "      p.data += g[n:n+num_elements].view(p.shape)\n",
    "      n += num_elements\n",
    "  train_rewards.append(np.mean(task_rewards))\n",
    "  print(f\"iter {e}: {np.mean(task_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_rollouts() got an unexpected keyword argument 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EVAL_EPOCHS):\n\u001b[1;32m      6\u001b[0m     policy\u001b[39m.\u001b[39mload_state_dict(original_params)\n\u001b[0;32m----> 7\u001b[0m     k_shot(policy, K, seed\u001b[39m=\u001b[39mSEED)\n\u001b[1;32m      8\u001b[0m     state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset(UNITS_TO_SELL, HORIZON, seed\u001b[39m=\u001b[39mSEED)\n\u001b[1;32m      9\u001b[0m     state \u001b[39m=\u001b[39m parse_state(state)\n",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m, in \u001b[0;36mk_shot\u001b[0;34m(policy, n_steps, seed, test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mk_shot\u001b[39m(policy, n_steps, seed, test\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m   \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_steps):\n\u001b[0;32m----> 3\u001b[0m     trajs \u001b[39m=\u001b[39m sample_rollouts(policy, env, \u001b[39m2\u001b[39m, seed\u001b[39m=\u001b[39mseed, test\u001b[39m=\u001b[39mtest)\n\u001b[1;32m      4\u001b[0m     u \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([grad_U_tau(policy, tau, GAMMA) \u001b[39mfor\u001b[39;00m tau \u001b[39min\u001b[39;00m trajs])\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m     g \u001b[39m=\u001b[39m u \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m EPSILON \u001b[39m/\u001b[39m (u \u001b[39m@\u001b[39m u))\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_rollouts() got an unexpected keyword argument 'test'"
     ]
    }
   ],
   "source": [
    "EVAL_EPOCHS = 50\n",
    "K = 2\n",
    "\n",
    "same_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    policy.load_state_dict(original_params)\n",
    "    k_shot(policy, K, seed=SEED)\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    same_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg same mdp eval reward: {np.mean(same_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train mdp eval reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "train_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    policy.load_state_dict(original_params)\n",
    "    k_shot(policy, K, seed=(BATCH_SIZE * EPOCHS)+e)\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=(BATCH_SIZE * EPOCHS)+e)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    train_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg train mdp eval reward: {np.mean(train_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test mdp eval reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "test_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    policy.load_state_dict(original_params)\n",
    "    k_shot(env, policy, K, seed=SEED+e, test=True)\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED+e, test=True)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    test_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg test mdp eval reward: {np.mean(test_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "EXP_NAME = \"maml_restricted_policy_update_fine_tune_mdp_train.json\"\n",
    "with open(f\"./results/{EXP_NAME}\", \"w+\") as f:\n",
    "    json.dump({\n",
    "        \"train_rewards\": train_rewards,\n",
    "        \"same_mdp_eval_rewards\": same_mdp_eval_rewards,\n",
    "        \"train_mdp_eval_rewards\": train_mdp_eval_rewards,\n",
    "        \"test_mdp_eval_rewards\": test_mdp_eval_rewards\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_params = policy.state_dict().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.1004]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tau = []\n",
    "state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED-4)\n",
    "state = parse_state(state)\n",
    "done = False\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        probs = policy(state)\n",
    "        action = torch.argmax(probs).item()\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = parse_state(next_state)\n",
    "    tau.append((state, action, reward))\n",
    "    state = next_state\n",
    "states, actions, rewards = zip(*tau)\n",
    "states = torch.cat(states)\n",
    "actions = torch.tensor([actions])\n",
    "rewards = torch.tensor([rewards])\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shot(policy, n_steps, seed, test=False):\n",
    "  for e in range(n_steps):\n",
    "    trajs = sample_rollouts(policy, env, 2, seed=seed, test=test)\n",
    "    u = torch.stack([grad_U_tau(policy, tau, GAMMA) for tau in trajs]).mean(axis=0)\n",
    "    g = u * torch.sqrt(2 * EPSILON / (u @ u))\n",
    "    n = 0\n",
    "    for i, p in enumerate(policy.parameters()):\n",
    "      num_elements = p.numel()\n",
    "      p.data += g[n:n+num_elements].view(p.shape)\n",
    "      n += num_elements\n",
    "    policy.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.1004]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tau = []\n",
    "state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED-4)\n",
    "state = parse_state(state)\n",
    "done = False\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        probs = policy(state)\n",
    "        action = torch.argmax(probs).item()\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = parse_state(next_state)\n",
    "    tau.append((state, action, reward))\n",
    "    state = next_state\n",
    "states, actions, rewards = zip(*tau)\n",
    "states = torch.cat(states)\n",
    "actions = torch.tensor([actions])\n",
    "rewards = torch.tensor([rewards])\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('cs238')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b0a38c3e275318a53fcb279e5fdf7a9fef28081cd7e4603c16e0b245341a686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
