{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "from mdp import TradeExecutionEnv, DiscreteTradeSizeWrapper\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "HORIZON = 5 * 12 * 8\n",
    "#HORIZON = 10\n",
    "UNITS_TO_SELL = 40\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1000\n",
    "EPSILON = 0.5\n",
    "GAMMA = 0.99\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "env = TradeExecutionEnv()\n",
    "\n",
    "trade_sizes = {\n",
    "  0: 0,\n",
    "  1: 1,\n",
    "  2: 2,\n",
    "  3: 4,\n",
    "  4: 8,\n",
    "  #5: 16,\n",
    "  #6: 32,\n",
    "  #7: 64,\n",
    "  #8: 128,\n",
    "  #9: 250\n",
    "}\n",
    "env = DiscreteTradeSizeWrapper(env, trade_sizes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(state):\n",
    "    data = torch.FloatTensor(np.stack([\n",
    "        state[\"low\"].to_numpy(),\n",
    "        state[\"high\"].to_numpy(),\n",
    "        state[\"close\"].to_numpy(),\n",
    "        state[\"open\"].to_numpy(),\n",
    "        state[\"volume\"].to_numpy(),\n",
    "    ])).T\n",
    "    return torch.concat([\n",
    "        data,\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"units_sold\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"cost_basis\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"steps_left\"]]]), 6, 0),\n",
    "    ], dim=1)[-1,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rollouts(policy, env, num_rollouts):\n",
    "    trajs = []\n",
    "    for _ in range(num_rollouts):\n",
    "        tau = []\n",
    "        state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED)\n",
    "        state = parse_state(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = parse_state(next_state)\n",
    "            tau.append((state, action, reward))\n",
    "            state = next_state\n",
    "        states, actions, rewards = zip(*tau)\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor([actions])\n",
    "        rewards = torch.tensor([rewards])\n",
    "        trajs.append((states, actions, rewards))\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_tau(r, gamma):\n",
    "    gammas = torch.tensor([gamma**i for i in range(len(r))])\n",
    "    return torch.sum(gammas * r)\n",
    "\n",
    "def avg_batch_rewards(trajs, gamma):\n",
    "    return torch.mean(torch.stack([R_tau(r, gamma) for _, _, r in trajs]))\n",
    "\n",
    "def grad_log_pi(policy, states, actions):\n",
    "    logits = policy(states)\n",
    "    log_probs = torch.log(logits)\n",
    "    log_probs = log_probs.gather(1,actions).T.flatten()\n",
    "    g = [torch.autograd.grad(log_p, policy.parameters(), retain_graph=True) for log_p in log_probs]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_tau(policy, tau):\n",
    "  g = grad_log_pi(policy, tau[0], tau[1])\n",
    "  stacked_g = [torch.stack(g_).view(len(tau[0]), -1) for g_ in zip(*g)]\n",
    "  return torch.cat(stacked_g, axis=1).sum(axis=0)\n",
    "\n",
    "def grad_U_tau(policy, tau, gamma):\n",
    "  return grad_log_tau(policy, tau) * R_tau(tau[2], gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: -8.774109746045383\n"
     ]
    }
   ],
   "source": [
    "policy = CategoricalPolicy(8, 5).to(device)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "  trajs = sample_rollouts(policy, env, BATCH_SIZE)\n",
    "  g_u_tau = [grad_U_tau(policy, tau, GAMMA) for tau in trajs]\n",
    "  Fish = torch.stack([gut.unsqueeze(1) @ gut.unsqueeze(0) for gut in g_u_tau]).mean(axis=0)\n",
    "  g_u = torch.stack(g_u_tau).mean(axis=0)\n",
    "  u = torch.linalg.pinv(Fish) @ g_u.unsqueeze(0).T\n",
    "  #u = torch.linalg.inv(Fish + torch.FloatTensor(np.diag([0.1] * Fish.shape[0]))) @ g_u.unsqueeze(0).T\n",
    "  g = u * torch.sqrt(2 * EPSILON / (g_u.T @ u))\n",
    "  #print(f\"G: {g}\")\n",
    "  n = 0\n",
    "  for i, p in enumerate(policy.parameters()):\n",
    "    num_elements = p.numel()\n",
    "    p.data += g[n:n+num_elements].view(p.shape)\n",
    "    n += num_elements\n",
    "  policy.zero_grad()\n",
    "  if e % 10 == 0:\n",
    "    print(f\"iter {e}: {avg_batch_rewards(trajs, GAMMA)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('cs238')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b0a38c3e275318a53fcb279e5fdf7a9fef28081cd7e4603c16e0b245341a686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
