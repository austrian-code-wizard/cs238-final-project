{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "from mdp import TradeExecutionEnv, DiscreteTradeSizeWrapper, RelativeTradeSizeWrapper\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "env = TradeExecutionEnv()\n",
    "\n",
    "SEED = 42\n",
    "HORIZON = 5 * 12 * 2\n",
    "UNITS_TO_SELL = 64\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "EPSILON = 0.01\n",
    "GAMMA = 0.99\n",
    "ALPHA = 0.9\n",
    "\n",
    "env = TradeExecutionEnv()\n",
    "\n",
    "trade_sizes = {\n",
    "  i: i*2 for i in range(33)\n",
    "}\n",
    "env = DiscreteTradeSizeWrapper(env, trade_sizes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32) -> None:\n",
    "        super().__init__()\n",
    "        #self.rnn = nn.LSTM(num_states, hidden_dim, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x, _ = self.rnn(x)\n",
    "        x = F.relu(self.fc1(x[:, -1, :]))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(state):\n",
    "    data = torch.FloatTensor(np.stack([\n",
    "        state[\"low\"].to_numpy(),\n",
    "        state[\"high\"].to_numpy(),\n",
    "        state[\"close\"].to_numpy(),\n",
    "        state[\"open\"].to_numpy(),\n",
    "        state[\"volume\"].to_numpy(),\n",
    "    ])).T\n",
    "    return torch.concat([\n",
    "        data,\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"units_sold\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"cost_basis\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"steps_left\"]]]), 6, 0),\n",
    "    ], dim=1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rollouts(policy, env, num_rollouts,seed=SEED):\n",
    "    trajs = []\n",
    "    for _ in range(num_rollouts):\n",
    "        tau = []\n",
    "        state = env.reset(UNITS_TO_SELL, HORIZON, seed=(seed * num_rollouts) + e)\n",
    "        state = parse_state(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = parse_state(next_state)\n",
    "            tau.append((state, action, reward))\n",
    "            state = next_state\n",
    "        states, actions, rewards = zip(*tau)\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor([actions])\n",
    "        rewards = torch.tensor([rewards])\n",
    "        trajs.append((states, actions, rewards))\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_tau(r):\n",
    "    gammas = torch.tensor([GAMMA**i for i in range(len(r))])\n",
    "    return torch.sum(gammas * r)\n",
    "\n",
    "def avg_batch_rewards(trajs):\n",
    "    return torch.mean(torch.stack([R_tau(r) for _, _, r in trajs]))\n",
    "\n",
    "def grad_log_pi(policy, states, actions):\n",
    "    logits = policy(states)\n",
    "    log_probs = torch.log(logits)\n",
    "    log_probs = log_probs.gather(1,actions).T.flatten()\n",
    "    g = [torch.autograd.grad(log_p, policy.parameters(), retain_graph=True) for log_p in log_probs]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_tau(policy, tau):\n",
    "  g = grad_log_pi(policy, tau[0], tau[1])\n",
    "  stacked_g = [torch.stack(g_).view(len(tau[0]), -1) for g_ in zip(*g)]\n",
    "  return torch.cat(stacked_g, axis=1).sum(axis=0)\n",
    "\n",
    "def grad_U_tau(policy, tau):\n",
    "  return grad_log_tau(policy, tau) * R_tau(tau[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_objective(policy, new_policy, trajs):\n",
    "    means = []\n",
    "    for tau in trajs:\n",
    "      states, actions, rewards = tau\n",
    "      prob_ratios = new_policy(states).gather(1, actions).T.flatten() / policy(states).gather(1, actions).T.flatten()\n",
    "      discounted_rewards = torch.FloatTensor([torch.sum(torch.stack([rewards.T[j] * GAMMA ** (j-i) for j in range(i,len(rewards.T))])) for i in range(len(rewards.T))])\n",
    "      means.append(torch.mean(prob_ratios * discounted_rewards))\n",
    "    return torch.mean(torch.stack(means))\n",
    "\n",
    "def surrogate_constraint(policy, new_policy, trajs):\n",
    "    means = []\n",
    "    for tau in trajs:\n",
    "        states, _, rewards = tau\n",
    "        discounted_rewards = GAMMA ** torch.arange(len(rewards.T))\n",
    "        policy_probs = policy(states)\n",
    "        #print(f\"Policy probs shape: {policy_probs.shape}\")\n",
    "        new_policy_probs = new_policy(states)\n",
    "        kl_divs = torch.sum(policy_probs * torch.log(policy_probs / new_policy_probs), axis=1)\n",
    "        #kl_divs = torch.sum(torch.kl_div(torch.log(policy_probs), new_policy_probs), axis=1)\n",
    "        means.append(torch.mean(kl_divs * discounted_rewards))\n",
    "    return torch.mean(torch.stack(means))\n",
    "\n",
    "def flatten_params(policy):\n",
    "    return torch.cat([p.view(-1) for p in policy.parameters()])\n",
    "\n",
    "def linesearch(policy, new_policy, trajs):\n",
    "    f_theta = surrogate_objective(policy, policy, trajs)\n",
    "    while surrogate_constraint(policy, new_policy, trajs) > EPSILON or surrogate_objective(policy, new_policy, trajs) <= f_theta:\n",
    "        theta = flatten_params(policy)\n",
    "        theta_new = flatten_params(new_policy)\n",
    "        theta_new = theta + ALPHA * (theta_new - theta)\n",
    "        n = 0\n",
    "        for _, p in enumerate(new_policy.parameters()):\n",
    "            num_elements = p.numel()\n",
    "            p.data = theta_new[n:n+num_elements].view(p.shape)\n",
    "            n += num_elements\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: -10.0\n",
      "iter 10: -9.393252309956434\n",
      "iter 20: -8.86914494946309\n",
      "iter 30: -9.437237867071891\n",
      "iter 40: -9.399950029675107\n",
      "iter 50: -10.0\n",
      "iter 60: -8.767210509025471\n",
      "iter 70: -10.0\n",
      "iter 80: -8.291639638446076\n",
      "iter 90: -9.414105959690371\n",
      "iter 100: -9.379112390318575\n",
      "iter 110: -10.0\n",
      "iter 120: -8.815065507185732\n",
      "iter 130: -9.429459304404334\n",
      "iter 140: -9.387194880073512\n",
      "iter 150: -9.434178384303358\n",
      "iter 160: -9.434908836327125\n",
      "iter 170: -9.398376568917149\n",
      "iter 180: -10.0\n",
      "iter 190: -9.42385530719504\n",
      "iter 200: -8.871562426470588\n",
      "iter 210: -9.383873889711387\n",
      "iter 220: -10.0\n",
      "iter 230: -8.301330695312501\n",
      "iter 240: -10.0\n",
      "iter 250: -8.775769866890482\n",
      "iter 260: -9.381359355316867\n",
      "iter 270: -8.806608376338605\n",
      "iter 280: -9.41252913751836\n",
      "iter 290: -8.873009903055152\n"
     ]
    }
   ],
   "source": [
    "policy = CategoricalPolicy(len(env.observation_space), len(trade_sizes)).to(device)\n",
    "\n",
    "train_rewards = []\n",
    "for e in range(EPOCHS):\n",
    "  trajs = sample_rollouts(policy, env, BATCH_SIZE,seed=e)\n",
    "  g_u_tau = [grad_U_tau(policy, tau) for tau in trajs]\n",
    "  Fish = torch.stack([gut.unsqueeze(1) @ gut.unsqueeze(0) for gut in g_u_tau]).mean(axis=0)\n",
    "  g_u = torch.stack(g_u_tau).mean(axis=0)\n",
    "  u = torch.linalg.pinv(Fish) @ g_u.unsqueeze(0).T\n",
    "  g = u * torch.sqrt(2 * EPSILON / (g_u @ u))\n",
    "  #print(f\"G: {g}\")\n",
    "  n = 0\n",
    "  new_policy = CategoricalPolicy(len(env.observation_space), len(trade_sizes)).to(device)\n",
    "  new_policy.load_state_dict(policy.state_dict().copy())\n",
    "  for i, p in enumerate(new_policy.parameters()):\n",
    "    num_elements = p.numel()\n",
    "    p.data += g[n:n+num_elements].view(p.shape)\n",
    "    n += num_elements\n",
    "  policy = linesearch(policy, new_policy, trajs)\n",
    "  policy.zero_grad()\n",
    "  train_rewards.append(avg_batch_rewards(trajs))\n",
    "  if e % 10 == 0:\n",
    "    print(f\"iter {e}: {avg_batch_rewards(trajs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg same mdp eval reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "EVAL_EPOCHS = 50\n",
    "\n",
    "same_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    same_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg same mdp eval reward: {np.mean(same_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train mdp eval reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "train_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=(BATCH_SIZE * EPOCHS)+e)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    train_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg train mdp eval reward: {np.mean(train_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg test mdp eval reward: -10.0\n"
     ]
    }
   ],
   "source": [
    "test_mdp_eval_rewards = []\n",
    "for e in range(EVAL_EPOCHS):\n",
    "    state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED+e, test=True)\n",
    "    state = parse_state(state)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action = torch.argmax(policy(state)).item()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = parse_state(next_state)\n",
    "        state = next_state\n",
    "        episode_rewards += reward\n",
    "    test_mdp_eval_rewards.append(episode_rewards)\n",
    "print(f\"Avg test mdp eval reward: {np.mean(test_mdp_eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "EXP_NAME = \"trpo_multi_mdp_train.json\"\n",
    "with open(f\"./results/{EXP_NAME}\", \"w+\") as f:\n",
    "    json.dump({\n",
    "        \"train_rewards\": train_rewards,\n",
    "        \"same_mdp_eval_rewards\": same_mdp_eval_rewards,\n",
    "        \"train_mdp_eval_rewards\": train_mdp_eval_rewards,\n",
    "        \"test_mdp_eval_rewards\": test_mdp_eval_rewards\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rewards = [n.item() for n in train_rewards]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('cs238')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b0a38c3e275318a53fcb279e5fdf7a9fef28081cd7e4603c16e0b245341a686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
