{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "from mdp import TradeExecutionEnv, DiscreteTradeSizeWrapper\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "HORIZON = 5 * 12 * 8\n",
    "#HORIZON = 10\n",
    "UNITS_TO_SELL = 40\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "EPSILON = 0.5\n",
    "ALPHA = 0.1\n",
    "GAMMA = 0.99\n",
    "TEMPERATURE = 0.1\n",
    "\n",
    "env = TradeExecutionEnv()\n",
    "\n",
    "trade_sizes = {\n",
    "  0: 0,\n",
    "  1: 1,\n",
    "  2: 2,\n",
    "  3: 4,\n",
    "  4: 8,\n",
    "  #5: 16,\n",
    "  #6: 32,\n",
    "  #7: 64,\n",
    "  #8: 128,\n",
    "  #9: 250\n",
    "}\n",
    "env = DiscreteTradeSizeWrapper(env, trade_sizes)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, num_states, num_actions, hidden_dim=32) -> None:\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_states, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state):\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_state(state):\n",
    "    data = torch.FloatTensor(np.stack([\n",
    "        state[\"low\"].to_numpy(),\n",
    "        state[\"high\"].to_numpy(),\n",
    "        state[\"close\"].to_numpy(),\n",
    "        state[\"open\"].to_numpy(),\n",
    "        state[\"volume\"].to_numpy(),\n",
    "    ])).T\n",
    "    return torch.concat([\n",
    "        data,\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"units_sold\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"cost_basis\"]]]), 6, 0),\n",
    "        torch.repeat_interleave(torch.FloatTensor([[state[\"steps_left\"]]]), 6, 0),\n",
    "    ], dim=1)[-1,:].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rollouts(policy, env, num_rollouts):\n",
    "    trajs = []\n",
    "    for _ in range(num_rollouts):\n",
    "        tau = []\n",
    "        state = env.reset(UNITS_TO_SELL, HORIZON, seed=SEED)\n",
    "        state = parse_state(state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                action, _ = policy.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = parse_state(next_state)\n",
    "            tau.append((state, action, reward))\n",
    "            state = next_state\n",
    "        states, actions, rewards = zip(*tau)\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor([actions])\n",
    "        rewards = torch.tensor([rewards])\n",
    "        trajs.append((states, actions, rewards))\n",
    "    return trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_tau(r):\n",
    "    gammas = torch.tensor([GAMMA**i for i in range(len(r))])\n",
    "    return torch.sum(gammas * r)\n",
    "\n",
    "def avg_batch_rewards(trajs):\n",
    "    return torch.mean(torch.stack([R_tau(r) for _, _, r in trajs]))\n",
    "\n",
    "def grad_log_pi(policy, states, actions):\n",
    "    logits = policy(states)\n",
    "    log_probs = torch.log(logits)\n",
    "    log_probs = log_probs.gather(1,actions).T.flatten()\n",
    "    g = [torch.autograd.grad(log_p, policy.parameters(), retain_graph=True) for log_p in log_probs]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_log_tau(policy, tau):\n",
    "  g = grad_log_pi(policy, tau[0], tau[1])\n",
    "  stacked_g = [torch.stack(g_).view(len(tau[0]), -1) for g_ in zip(*g)]\n",
    "  return torch.cat(stacked_g, axis=1).sum(axis=0)\n",
    "\n",
    "def grad_U_tau(policy, tau):\n",
    "  return grad_log_tau(policy, tau) * R_tau(tau[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_objective(policy, new_policy, trajs):\n",
    "    means = []\n",
    "    for tau in trajs:\n",
    "      states, actions, rewards = tau\n",
    "      prob_ratios = new_policy(states).gather(1, actions).T.flatten() / policy(states).gather(1, actions).T.flatten()\n",
    "      discounted_rewards = torch.FloatTensor([torch.sum(torch.stack([rewards.T[j] * GAMMA ** (j-i) for j in range(i,len(rewards.T))])) for i in range(len(rewards.T))])\n",
    "      means.append(torch.mean(prob_ratios * discounted_rewards))\n",
    "    return torch.mean(torch.stack(means))\n",
    "\n",
    "def surrogate_constraint(policy, new_policy, trajs):\n",
    "    means = []\n",
    "    for tau in trajs:\n",
    "        states, _, rewards = tau\n",
    "        discounted_rewards = GAMMA ** torch.arange(len(rewards.T))\n",
    "        policy_probs = policy(states)\n",
    "        #print(f\"Policy probs shape: {policy_probs.shape}\")\n",
    "        new_policy_probs = new_policy(states)\n",
    "        kl_divs = torch.sum(policy_probs * torch.log(policy_probs / new_policy_probs), axis=1)\n",
    "        means.append(torch.mean(kl_divs * discounted_rewards))\n",
    "    return torch.mean(torch.stack(means))\n",
    "\n",
    "def flatten_params(policy):\n",
    "    params = torch.cat([p.view(-1) for p in policy.parameters()])\n",
    "    print(f\"Params shape {params.shape}\")\n",
    "    return params\n",
    "\n",
    "def linesearch(policy, new_policy, trajs):\n",
    "    f_theta = surrogate_objective(policy, policy, trajs)\n",
    "    while surrogate_constraint(policy, new_policy, trajs) > EPSILON or surrogate_objective(policy, new_policy, trajs) < f_theta:\n",
    "        theta = flatten_params(policy)\n",
    "        theta_new = flatten_params(new_policy)\n",
    "        theta_new = theta + ALPHA * (theta_new - theta)\n",
    "        n = 0\n",
    "        for _, p in enumerate(new_policy.parameters()):\n",
    "            num_elements = p.numel()\n",
    "            p.data = theta_new[n:n+num_elements].view(p.shape)\n",
    "            n += num_elements\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: -8.163849246412052\n"
     ]
    }
   ],
   "source": [
    "policy = CategoricalPolicy(8, 5).to(device)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "  trajs = sample_rollouts(policy, env, BATCH_SIZE)\n",
    "  g_u_tau = [grad_U_tau(policy, tau) for tau in trajs]\n",
    "  Fish = torch.stack([gut.unsqueeze(1) @ gut.unsqueeze(0) for gut in g_u_tau]).mean(axis=0)\n",
    "  g_u = torch.stack(g_u_tau).mean(axis=0)\n",
    "  u = torch.linalg.pinv(Fish) @ g_u.unsqueeze(0).T\n",
    "  g = u * torch.sqrt(2 * EPSILON / (g_u @ u))\n",
    "  #print(f\"G: {g}\")\n",
    "  n = 0\n",
    "  new_policy = CategoricalPolicy(8, 5).to(device)\n",
    "  new_policy.load_state_dict(policy.state_dict())\n",
    "  for i, p in enumerate(new_policy.parameters()):\n",
    "    num_elements = p.numel()\n",
    "    p.data += g[n:n+num_elements].view(p.shape)\n",
    "    n += num_elements\n",
    "  policy = linesearch(policy, new_policy, trajs)\n",
    "  policy.zero_grad()\n",
    "  if e % 10 == 0:\n",
    "    print(f\"iter {e}: {avg_batch_rewards(trajs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('cs238')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b0a38c3e275318a53fcb279e5fdf7a9fef28081cd7e4603c16e0b245341a686"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
